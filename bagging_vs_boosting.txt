>>boosting vs bagging 
1.if the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, 
Boosting could generate a combined model with lower errors as it optimises the advantages and reduces the pitfalls of the single model.
By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn’t help to
avoid over-fitting; in fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting.

2.These two are the most important terms describing the ensemble (combination) of various models into one more effective model.
Most of the errors from a model’s learning are from three main factors: variance, noise, and bias. By using ensemble methods, 
we’re able to increase the stability of the final model and reduce the errors mentioned above.
a.Bagging to decrease the model’s variance.(solution for overfitting)
b.Boosting to decreasing the model’s bias.(better for increasing accuracy)

A)Bagging:
we use bootstrapping technique here.Bootstrapping is a sampling technique in which we create subsets of observations from 
the original dataset, with replacement.
steps in creating ensemble model using bagging:
1.Multiple subsets are created from the original dataset, selecting observations with replacement.
2.A base model (weak model) is created on each of these subsets.
3.The models run in parallel and are independent of each other.
4.The final predictions are determined by combining the predictions from all the models.

B)Boosting:
Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. 
The succeeding models are dependent on the previous model.steps:
1.A subset is created from the original dataset.
2.Initially, all data points are given equal weights.
3.A base model is created on this subset.
4.This model is used to make predictions on the whole dataset.
5. Errors are calculated using the actual values and predicted values.
6. The observations which are incorrectly predicted, are given higher weights.
7. Another model is created and predictions are made on the dataset.(This model tries to correct the errors from the previous model)
8. Similarly, multiple models are created, each correcting the errors of the previous model.
9. The final model (strong learner) is the weighted mean of all the models (weak learners).
Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on 
the entire dataset,but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.
