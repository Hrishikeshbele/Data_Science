> A confusion matrix can be used to measure the performance of a particular classifier with a fixed threshold/decision boundary. 
> A confusion matrix evaluates one particular classifier with a fixed threshold, while the AUC evaluates that classifier over all possible thresholds.

1.ROC Curve:
ROC is a plot of signal (True Positive Rate) against noise (False Positive Rate). The model performance is determined by looking at the area under the ROC curve (or AUC). The best possible AUC is 1 while the worst is 0.5.it can be used for parameter tuning.
>The ROC curve, examines the performance of a classifier with varying the threshold.The ROC curve is then generated by testing every possible threshold and plotting each result as a point on the curve.with varying threshold values in confusion matrix change as result fpr
and tpr changes.

ex. fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred) #this implementation is restricted to the binary classification task
    #computes auc 
    roc_auc = metrics.auc(fpr, tpr)
    >>0.75
2.accuracy:
ex.#score calculates the accuracy of classifier
    clf.score( X_test,y_test)
3.log loss:
it is calculated as log(y_test)-log(y_pred)
ex. sklearn.metrics.log_loss(y_test,y_pred)
